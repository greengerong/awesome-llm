{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greengerong/awesome-llm/blob/main/colab/green_potat1_text_to_video_colab_ALM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1 (Mandatory) - Install Txt to Video Finetuning"
      ],
      "metadata": {
        "id": "ms6hHpqh9gw7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2ibl97e2yM4",
        "outputId": "4f217192-1ad3-4157-b762-0de4c858ca3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m665.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 5, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 10, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n",
            "    from pip._internal.build_env import get_runnable_pip\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/build_env.py\", line 15, in <module>\n",
            "    from pip._vendor.packaging.requirements import Requirement\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 23, in <module>\n",
            "    from .markers import MARKER_EXPR, Marker\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 23, in <module>\n",
            "    from .specifiers import InvalidSpecifier, Specifier\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/specifiers.py\", line 24, in <module>\n",
            "    from .utils import canonicalize_version\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/utils.py\", line 8, in <module>\n",
            "    from .tags import Tag, parse_tag\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/tags.py\", line 8, in <module>\n",
            "    import sysconfig\n",
            "  File \"/usr/lib/python3.10/sysconfig.py\", line 595, in <module>\n",
            "    def get_paths(scheme=get_default_scheme(), vars=None, expand=True):\n",
            "  File \"/usr/lib/python3.10/sysconfig.py\", line 297, in get_default_scheme\n",
            "    def get_default_scheme():\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Cloning into 'Text-To-Video-Finetuning'...\n",
            "remote: Enumerating objects: 973, done.\u001b[K\n",
            "remote: Counting objects: 100% (322/322), done.\u001b[K\n",
            "remote: Compressing objects: 100% (102/102), done.\u001b[K\n",
            "remote: Total 973 (delta 250), reused 230 (delta 220), pack-reused 651\u001b[K\n",
            "Receiving objects: 100% (973/973), 1.77 MiB | 23.56 MiB/s, done.\n",
            "Resolving deltas: 100% (571/571), done.\n",
            "Cloning into 'Potat1ALM'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 11 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (11/11), 5.05 KiB | 1.68 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "#AI Lost Media's Text to Video Colab Workspace https://youtube.com/@ailostmedia\n",
        "#huge thanks to Camenduru https://twitter.com/camenduru and Cerspense https://twitter.com/cerspense for putting these models together.\n",
        "#tutorial: https://www.ailostmedia.com/post/the-ai-lost-media-text-to-video-colab-workspace\n",
        "%cd /content\n",
        "!pip install -q torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 torchtext==0.14.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu116 -U\n",
        "!pip install git+https://github.com/huggingface/diffusers transformers accelerate imageio[ffmpeg] -U einops omegaconf decord xformers==0.0.16 safetensors\n",
        "!git clone -b dev https://github.com/camenduru/Text-To-Video-Finetuning\n",
        "!git clone https://github.com/ailostmedia/Potat1ALM\n",
        "!mv /content/Potat1ALM/inference.py /content/Text-To-Video-Finetuning/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2 (Mandatory) - Install Potat1 or ZeroScope (or both)"
      ],
      "metadata": {
        "id": "-QP-G6UZTv0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title STEP2: Install Potat1\n",
        "#default 1024 x 576 - try 800 x 448 for colab\n",
        "%cd /content/\n",
        "!git clone https://huggingface.co/camenduru/potat1"
      ],
      "metadata": {
        "id": "qA3tHp7PT6Iz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60722eb8-9fd7-4465-e12f-4eac0b9aff78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'potat1'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 88 (delta 38), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (88/88), 521.91 KiB | 6.00 MiB/s, done.\n",
            "Filtering content: 100% (3/3), 4.05 GiB | 58.96 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title STEP2: Install ZeroScope 576\n",
        "#default 576 x 320\n",
        "%cd /content/\n",
        "!git clone https://huggingface.co/cerspense/zeroscope_v2_576w"
      ],
      "metadata": {
        "id": "wynHDO_gjRVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3480609d-db27-4b85-b271-a5f30794112d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'zeroscope_v2_576w'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 46 (delta 13), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (46/46), 518.32 KiB | 4.18 MiB/s, done.\n",
            "Filtering content: 100% (5/5), 7.88 GiB | 47.01 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title STEP2: Install ZeroScope XL\n",
        "#default 1024 x 576 - try 800 x 448 for colab\n",
        "%cd /content/\n",
        "!git clone https://huggingface.co/cerspense/zeroscope_v2_XL"
      ],
      "metadata": {
        "id": "Q6UV03XCjR76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d147f25-98fb-4bfb-8548-97d8bab8521a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'zeroscope_v2_XL'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 58 (delta 17), reused 3 (delta 3), pack-reused 32\u001b[K\n",
            "Unpacking objects: 100% (58/58), 520.27 KiB | 8.39 MiB/s, done.\n",
            "Filtering content: 100% (5/5), 7.88 GiB | 45.12 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title STEP2: Install ZeroScope 448\n",
        "#default 448 x 256\n",
        "%cd /content/\n",
        "!git clone https://huggingface.co/cerspense/zeroscope_v2_dark_30x448x256"
      ],
      "metadata": {
        "id": "zIc8zOhYTtCQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c95e163-6b28-4aaa-cd13-13093e7b2683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'zeroscope_v2_dark_30x448x256'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 34 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (34/34), 516.88 KiB | 4.74 MiB/s, done.\n",
            "error: unable to write file zeroscope_v2_dark_30x448x256_text.bin\n",
            "Downloading unet/diffusion_pytorch_model.bin (2.8 GB)\n",
            "Error downloading object: unet/diffusion_pytorch_model.bin (25bdf6e): Smudge error: Error downloading unet/diffusion_pytorch_model.bin (25bdf6ea2b993809e0526108f77268eb8612d7985465c8bb4d78dbba76fce4bf): cannot write data to tempfile \"/content/zeroscope_v2_dark_30x448x256/.git/lfs/incomplete/25bdf6ea2b993809e0526108f77268eb8612d7985465c8bb4d78dbba76fce4bf287409533\": write /content/zeroscope_v2_dark_30x448x256/.git/lfs/incomplete/25bdf6ea2b993809e0526108f77268eb8612d7985465c8bb4d78dbba76fce4bf287409533: no space left on device\n",
            "Unable to log panic to /content/zeroscope_v2_dark_30x448x256/.git/lfs/logs: mkdir /content/zeroscope_v2_dark_30x448x256/.git/lfs/logs: no space left on device\n",
            "\n",
            "git-lfs/2.9.2 (GitHub; linux amd64; go 1.13.5)\n",
            "git version 2.25.1\n",
            "\n",
            "$ git-lfs filter-process\n",
            "Error downloading object: unet/diffusion_pytorch_model.bin (25bdf6e): Smudge error: Error downloading unet/diffusion_pytorch_model.bin (25bdf6ea2b993809e0526108f77268eb8612d7985465c8bb4d78dbba76fce4bf): cannot write data to tempfile \"/content/zeroscope_v2_dark_30x448x256/.git/lfs/incomplete/25bdf6ea2b993809e0526108f77268eb8612d7985465c8bb4d78dbba76fce4bf287409533\": write /content/zeroscope_v2_dark_30x448x256/.git/lfs/incomplete/25bdf6ea2b993809e0526108f77268eb8612d7985465c8bb4d78dbba76fce4bf287409533: no space left on device\n",
            "\n",
            "write /content/zeroscope_v2_dark_30x448x256/.git/lfs/incomplete/25bdf6ea2b993809e0526108f77268eb8612d7985465c8bb4d78dbba76fce4bf287409533: no space left on device\n",
            "cannot write data to tempfile \"/content/zeroscope_v2_dark_30x448x256/.git/lfs/incomplete/25bdf6ea2b993809e0526108f77268eb8612d7985465c8bb4d78dbba76fce4bf287409533\"\n",
            "github.com/git-lfs/git-lfs/errors.newWrappedError\n",
            "\tgithub.com/git-lfs/git-lfs/errors/types.go:198\n",
            "github.com/git-lfs/git-lfs/errors.Wrapf\n",
            "\tgithub.com/git-lfs/git-lfs/errors/errors.go:85\n",
            "github.com/git-lfs/git-lfs/tq.(*basicDownloadAdapter).download\n",
            "\tgithub.com/git-lfs/git-lfs/tq/basic_download.go:250\n",
            "github.com/git-lfs/git-lfs/tq.(*basicDownloadAdapter).DoTransfer\n",
            "\tgithub.com/git-lfs/git-lfs/tq/basic_download.go:101\n",
            "github.com/git-lfs/git-lfs/tq.(*adapterBase).worker\n",
            "\tgithub.com/git-lfs/git-lfs/tq/adapterbase.go:182\n",
            "runtime.goexit\n",
            "\t/usr/lib/go-1.13/src/runtime/asm_amd64.s:1357\n",
            "Error downloading unet/diffusion_pytorch_model.bin (25bdf6ea2b993809e0526108f77268eb8612d7985465c8bb4d78dbba76fce4bf)\n",
            "github.com/git-lfs/git-lfs/errors.newWrappedError\n",
            "\tgithub.com/git-lfs/git-lfs/errors/types.go:198\n",
            "github.com/git-lfs/git-lfs/errors.Wrapf\n",
            "\tgithub.com/git-lfs/git-lfs/errors/errors.go:85\n",
            "github.com/git-lfs/git-lfs/lfs.(*GitFilter).downloadFile\n",
            "\tgithub.com/git-lfs/git-lfs/lfs/gitfilter_smudge.go:115\n",
            "github.com/git-lfs/git-lfs/lfs.(*GitFilter).Smudge\n",
            "\tgithub.com/git-lfs/git-lfs/lfs/gitfilter_smudge.go:76\n",
            "github.com/git-lfs/git-lfs/commands.smudge\n",
            "\tgithub.com/git-lfs/git-lfs/commands/command_smudge.go:127\n",
            "github.com/git-lfs/git-lfs/commands.filterCommand\n",
            "\tgithub.com/git-lfs/git-lfs/commands/command_filter_process.go:118\n",
            "github.com/spf13/cobra.(*Command).execute\n",
            "\tgithub.com/spf13/cobra/command.go:766\n",
            "github.com/spf13/cobra.(*Command).ExecuteC\n",
            "\tgithub.com/spf13/cobra/command.go:850\n",
            "github.com/spf13/cobra.(*Command).Execute\n",
            "\tgithub.com/spf13/cobra/command.go:800\n",
            "github.com/git-lfs/git-lfs/commands.Run\n",
            "\tgithub.com/git-lfs/git-lfs/commands/run.go:97\n",
            "main.main\n",
            "\tgithub.com/git-lfs/git-lfs/git-lfs.go:33\n",
            "runtime.main\n",
            "\t/usr/lib/go-1.13/src/runtime/proc.go:203\n",
            "runtime.goexit\n",
            "\t/usr/lib/go-1.13/src/runtime/asm_amd64.s:1357\n",
            "Smudge error\n",
            "github.com/git-lfs/git-lfs/errors.newWrappedError\n",
            "\tgithub.com/git-lfs/git-lfs/errors/types.go:198\n",
            "github.com/git-lfs/git-lfs/errors.NewSmudgeError\n",
            "\tgithub.com/git-lfs/git-lfs/errors/types.go:284\n",
            "github.com/git-lfs/git-lfs/lfs.(*GitFilter).Smudge\n",
            "\tgithub.com/git-lfs/git-lfs/lfs/gitfilter_smudge.go:85\n",
            "github.com/git-lfs/git-lfs/commands.smudge\n",
            "\tgithub.com/git-lfs/git-lfs/commands/command_smudge.go:127\n",
            "github.com/git-lfs/git-lfs/commands.filterCommand\n",
            "\tgithub.com/git-lfs/git-lfs/commands/command_filter_process.go:118\n",
            "github.com/spf13/cobra.(*Command).execute\n",
            "\tgithub.com/spf13/cobra/command.go:766\n",
            "github.com/spf13/cobra.(*Command).ExecuteC\n",
            "\tgithub.com/spf13/cobra/command.go:850\n",
            "github.com/spf13/cobra.(*Command).Execute\n",
            "\tgithub.com/spf13/cobra/command.go:800\n",
            "github.com/git-lfs/git-lfs/commands.Run\n",
            "\tgithub.com/git-lfs/git-lfs/commands/run.go:97\n",
            "main.main\n",
            "\tgithub.com/git-lfs/git-lfs/git-lfs.go:33\n",
            "runtime.main\n",
            "\t/usr/lib/go-1.13/src/runtime/proc.go:203\n",
            "runtime.goexit\n",
            "\t/usr/lib/go-1.13/src/runtime/asm_amd64.s:1357\n",
            "\n",
            "Current time in UTC: \n",
            "2023-07-08 11:39:46\n",
            "\n",
            "ENV:\n",
            "LocalWorkingDir=/content/zeroscope_v2_dark_30x448x256\n",
            "LocalGitDir=/content/zeroscope_v2_dark_30x448x256/.git\n",
            "LocalGitStorageDir=/content/zeroscope_v2_dark_30x448x256/.git\n",
            "LocalMediaDir=/content/zeroscope_v2_dark_30x448x256/.git/lfs/objects\n",
            "LocalReferenceDirs=\n",
            "TempDir=/content/zeroscope_v2_dark_30x448x256/.git/lfs/tmp\n",
            "ConcurrentTransfers=3\n",
            "TusTransfers=false\n",
            "BasicTransfersOnly=false\n",
            "SkipDownloadErrors=false\n",
            "FetchRecentAlways=false\n",
            "FetchRecentRefsDays=7\n",
            "FetchRecentCommitsDays=0\n",
            "FetchRecentRefsIncludeRemotes=true\n",
            "PruneOffsetDays=3\n",
            "PruneVerifyRemoteAlways=false\n",
            "PruneRemoteName=origin\n",
            "LfsStorageDir=/content/zeroscope_v2_dark_30x448x256/.git/lfs\n",
            "AccessDownload=none\n",
            "AccessUpload=none\n",
            "DownloadTransfers=basic,lfs-standalone-file\n",
            "UploadTransfers=basic,lfs-standalone-file\n",
            "GIT_EXEC_PATH=/usr/lib/git-core\n",
            "GIT_DIR=/content/zeroscope_v2_dark_30x448x256/.git\n",
            "GIT_PAGER=cat\n",
            "\n",
            "Client IP addresses:\n",
            "172.28.0.12\n",
            "error: external filter 'git-lfs filter-process' failed\n",
            "fatal: unet/diffusion_pytorch_model.bin: smudge filter lfs failed\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3 - Text to Video"
      ],
      "metadata": {
        "id": "CZr0n9j10ixM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqo7g4p2_V78",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "705475e6-3e89-4111-a2dc-aa6ea39372cb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-05d795b8f5e3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/Text-To-Video-Finetuning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-85>\u001b[0m in \u001b[0;36mcd\u001b[0;34m(self, parameter_s)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py\u001b[0m in \u001b[0;36mcd\u001b[0;34m(self, parameter_s)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moldcwd\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[0mdhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dhist'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompress_dhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pickleshare.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# We specify protocol 2, so that we can mostly go between Python 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# and Python 3. We can upgrade to protocol 3 when Python 2 is obsolete.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mfil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m         return self._accessor.open(self, mode, buffering, encoding, errors,\n\u001b[0m\u001b[1;32m   1120\u001b[0m                                    newline)\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '/root/.ipython/profile_default/db/dhist'"
          ]
        }
      ],
      "source": [
        "%cd /content/Text-To-Video-Finetuning\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\"\"\"\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic=True\n",
        "random.seed(2)\n",
        "np.random.seed(2)\n",
        "torch.manual_seed(2)\n",
        "torch.cuda.manual_seed(2)\n",
        "torch.cuda.manual_seed_all(2)\n",
        "torch.manual_seed(0)\n",
        "\"\"\"\n",
        "#print(\"seed is \" + str(torch.seed()))\n",
        "\n",
        "#seeding = \"Random\"\n",
        "#thisSeed = 123;\n",
        "\n",
        "\n",
        "#preset = \"Manual\"\n",
        "# while True:\n",
        "#@markdown #### Be sure you have installed the model you want in step 2\n",
        "model = \"potat1\" #@param [\"potat1\", \"zeroscope_v2_dark_30x448x256\", \"zeroscope_v2_576w\", \"zeroscope_v2_XL\"]\n",
        "prompt = \"extremely detailed, Futuristic Cityscape, blade runner, extremely cloudy, awardwinning, best quality, 8k\" #@param {type:\"string\"}\n",
        "negative = \"text, watermark, copyright, blurry, nsfw, noise, quick motion, bad quality, flicker, dirty, ugly, fast motion, quick cuts, fast editing, cuts\" #@param {type:\"string\"}\n",
        "prompt = f\"\\\"{prompt}\\\"\"\n",
        "negative = f\"\\\"{negative}\\\"\"\n",
        "num_steps = 25 #@param {type:\"raw\"}\n",
        "guidance_scale = 23 #@param {type:\"raw\"}\n",
        "width = 800 #@param {type:\"raw\"}\n",
        "height = 448 #@param {type:\"raw\"}\n",
        "fps = 10 #@param {type:\"raw\"}\n",
        "num_frames = 30 #@param {type:\"raw\"}\n",
        "seedManual = \"Random\"\n",
        "seeding = \"Random\" #@param [\"Random\", \"Manual\"]\n",
        "\n",
        "inputSeed = 7106521602475165645 #@param {type:\"raw\"}\n",
        "if seeding == \"Random\":\n",
        "  thisSeed = random.randint(0, ((1<<63)-1))\n",
        "  print(\"seed is \" + str(thisSeed))\n",
        "else:\n",
        "  thisSeed = inputSeed\n",
        "\n",
        "thisHeight = int(round(height/8.0)*8.0)\n",
        "thisWidth = int(round(width/8.0)*8.0)\n",
        "\n",
        "thisModel=\"/content/\"+model\n",
        "!python inference.py -m {thisModel} -p {prompt} -n {negative} -W {thisWidth} -H {thisHeight} -o /content/outputs -d cuda -x -s {num_steps} -g {guidance_scale} -f {fps} -T {num_frames} -seed {thisSeed}\n",
        "#-seed {thisSeed}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optional: Image to Vid"
      ],
      "metadata": {
        "id": "LbmrmwE13PqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Img2Vid Step 1: Install 3D Photo inpainting\n",
        "!apt -y install -qq aria2 xvfb\n",
        "!pip install vispy transforms3d networkx\n",
        "%cd /content/\n",
        "!git clone -b dev https://github.com/camenduru/3d-photo-inpainting\n",
        "%cd /content/3d-photo-inpainting\n",
        "!git clone https://github.com/camenduru/BoostingMonocularDepth\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/3d-photo-inpainting/resolve/main/color-model.pth -d /content/3d-photo-inpainting/checkpoints -o color-model.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/3d-photo-inpainting/resolve/main/depth-model.pth -d /content/3d-photo-inpainting/checkpoints -o depth-model.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/3d-photo-inpainting/resolve/main/edge-model.pth -d /content/3d-photo-inpainting/checkpoints -o edge-model.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/3d-photo-inpainting/resolve/main/model.pt -d /content/3d-photo-inpainting/checkpoints -o model.pt\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/3d-photo-inpainting/resolve/main/latest_net_G.pth -d /content/3d-photo-inpainting/BoostingMonocularDepth/pix2pix/checkpoints/mergemodel -o latest_net_G.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/3d-photo-inpainting/resolve/main/model-f46da743.pt -d /content/3d-photo-inpainting/BoostingMonocularDepth/midas -o model.pt"
      ],
      "metadata": {
        "id": "Eu3aWLCC3ZA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Img2Vid Step 2: Select Photo - Subject should be in center (use Jpg)\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "def reformat_photo(photo):\n",
        "    \"\"\"function to reformat photo\"\"\"\n",
        "\n",
        "\n",
        "#@markdown ### Select an uploading method\n",
        "upload_method = \"Upload\" #@param [\"Upload\", \"Custom Path\"]\n",
        "\n",
        "\n",
        "# remove previous input video\n",
        "if os.path.isfile('/content/3d-photo-inpainting/image/test.jpg'):\n",
        "    os.remove('/content/3d-photo-inpainting/image/test.jpg')\n",
        "\n",
        "if upload_method == \"Upload\":\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        os.rename(filename, '/content/3d-photo-inpainting/image/test.jpg')\n",
        "    PATH_TO_YOUR_PHOTO = '/content/3d-photo-inpainting/image/test.jpg'\n",
        "\n",
        "elif upload_method == 'Custom Path':\n",
        "    if not 'drive' in globals():\n",
        "        drive.mount('/content/drive')\n",
        "    #@markdown ``Add the full path to your video on your Gdrive `` üëá\n",
        "    PATH_TO_YOUR_PHOTO = '/content/3d-photo-inpainting/image/test.jpg' #@param {type:\"string\"}\n",
        "    if not os.path.isfile(PATH_TO_YOUR_PHOTO):\n",
        "        print(\"ERROR: File not found!\")\n",
        "        raise SystemExit(0)\n",
        "\n",
        "if upload_method == \"Upload\":\n",
        "  print(\"Input photo\")\n",
        "\n",
        "else:\n",
        "    if os.path.isfile(PATH_TO_YOUR_PHOTO):\n",
        "        shutil.copyfile(PATH_TO_YOUR_PHOTO, \"/content/3d-photo-inpainting/image/test.jpg\")\n",
        "        print(\"Input Photo\")\n",
        "        showVideo(PATH_TO_YOUR_PHOTO)"
      ],
      "metadata": {
        "id": "YSDV4Te3OLnL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Img2Vid Step 3: Run 3D Photo inpainting\n",
        "!pip install pyyaml\n",
        "#@markdown ### Select the video type\n",
        "video_type = \"zoom-in\" #@param [\"zoom-in\", \"dolly-zoom-in\", \"circle\", \"swing\"]\n",
        "#@markdown ### Select the trajectory type (straight line to end video after move, circle to end at start)\n",
        "trajectory_type = \"double-straight-line\" #@param [\"double-straight-line\", \"circle\"]\n",
        "#@markdown ### Input the trajectories (use between 0 and .05)\n",
        "#@markdown ### X (Negative is right, Positive is left)\n",
        "x = 0.00 #@param {type:\"raw\"}\n",
        "#@markdown ### Y (Negative is down, Positive is up)\n",
        "y = 0.015 #@param {type:\"raw\"}\n",
        "#@markdown ### Z (Negative is toward subject, Positive is away)\n",
        "z = -0.05 #@param {type:\"raw\"}\n",
        "#@markdown ### input the fps and frames for video\n",
        "fps = 24 #@param {type:\"raw\"}\n",
        "num_frames = 72 #@param {type:\"raw\"}\n",
        "\n",
        "import yaml\n",
        "%cd /content/3d-photo-inpainting\n",
        "\n",
        "with open('argument.yml', 'r') as file:\n",
        "  arguments = file.readlines()\n",
        "\n",
        "#yaml_data[5] = \"fps: \"+ str(fps)\n",
        "arguments[5] = \"fps: \"+ str(fps) +\"\\n\"\n",
        "arguments[6] = \"num_frames: \"+ str(num_frames) +\"\\n\"\n",
        "arguments[7] = \"x_shift_range: [\" +str(x) +\"]\\n\"\n",
        "arguments[8] = \"y_shift_range: [\" +str(y) +\"]\\n\"\n",
        "arguments[9] = \"z_shift_range: [\" +str(z) +\"]\\n\"\n",
        "arguments[10] = \"traj_types: [\" +str(trajectory_type) +\"]\\n\"\n",
        "arguments[11] = \"video_postfix: [\" +str(video_type) +\"]\\n\"\n",
        "\n",
        "with open('argument.yml', 'w') as file:\n",
        "  file.writelines(arguments)\n",
        "\n",
        "!xvfb-run -s \"-screen 0 1280x720x24\" python main.py --config argument.yml\n",
        "\n"
      ],
      "metadata": {
        "id": "JPWkWSeT-zkc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Img2Vid STEP 4: V2V with ZeroScope (Will Need Interpolating)\n",
        "%cd /content/Text-To-Video-Finetuning\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\"\"\"\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic=True\n",
        "random.seed(2)\n",
        "np.random.seed(2)\n",
        "torch.manual_seed(2)\n",
        "torch.cuda.manual_seed(2)\n",
        "torch.cuda.manual_seed_all(2)\n",
        "torch.manual_seed(0)\n",
        "\"\"\"\n",
        "#print(\"seed is \" + str(torch.seed()))\n",
        "\n",
        "#seeding = \"Random\"\n",
        "#thisSeed = 123;\n",
        "\n",
        "\n",
        "#preset = \"Manual\"\n",
        "# while True:\n",
        "#@markdown ### Select the model (be sure to have installed in Step 2)\n",
        "model = \"zeroscope_v2_dark_30x448x256\" #@param [\"potat1\", \"zeroscope_v2_dark_30x448x256\", \"zeroscope_v2_576w\", \"zeroscope_v2_XL\"]\n",
        "#@markdown ### Copy Video Path from 3d-photo-inpainting/video\n",
        "video_path = \"\" #@param {type:\"string\"}\n",
        "video_weight = .2 #@param {type:\"raw\"}\n",
        "#@markdown ### Fill in prompt and parameters!\n",
        "prompt = \"Extremely Detailed, dynamic shot, action, 80s live action medieval fantasy film shot of a young hero riding a hovercycle high in the clouds, stormy, hyperrealistic, best quality, awardwinning\" #@param {type:\"string\"}\n",
        "negative = \"blurry, text, watermark, copyright, blurry, nsfw, noise, quick motion, bad quality, flicker, dirty, ugly, fast motion, quick cuts, fast editing, cuts\" #@param {type:\"string\"}\n",
        "prompt = f\"\\\"{prompt}\\\"\"\n",
        "negative = f\"\\\"{negative}\\\"\"\n",
        "num_steps = 50 #@param {type:\"raw\"}\n",
        "guidance_scale = 23 #@param {type:\"raw\"}\n",
        "width = 448 #@param {type:\"raw\"}\n",
        "height = 256 #@param {type:\"raw\"}\n",
        "fps = 10 #@param {type:\"raw\"}\n",
        "num_frames = 30 #@param {type:\"raw\"}\n",
        "#@markdown ### Seeding not currently working for v2v (WIP)\n",
        "seedManual = \"Random\"\n",
        "seeding = \"Random\" #@param [\"Random\", \"Manual\"]\n",
        "\n",
        "inputSeed = 6708511088475640657 #@param {type:\"raw\"}\n",
        "if seeding == \"Random\":\n",
        "  thisSeed = random.randint(0, ((1<<63)-1))\n",
        "  print(\"seed is \" + str(thisSeed))\n",
        "else:\n",
        "  thisSeed = inputSeed\n",
        "\n",
        "thisHeight = int(round(height/8.0)*8.0)\n",
        "thisWidth = int(round(width/8.0)*8.0)\n",
        "thisModel=\"/content/\"+model\n",
        "thisVideoPath = str(video_path)\n",
        "!python inference.py -m {thisModel} -p {prompt} -n {negative} -W {thisWidth} -H {thisHeight} -o /content/outputs -d cuda -x -s {num_steps} -g {guidance_scale} -f {fps} -T {num_frames} -seed {thisSeed} -i {thisVideoPath} -iw {video_weight}\n",
        "#-seed {thisSeed}"
      ],
      "metadata": {
        "id": "xks-YDvld_3B",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optional - V2V (Upload your own or use ZeroScope below)"
      ],
      "metadata": {
        "id": "xCJ0J8ZDkLQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title V2V Step 1: Run ZeroScope (Optional)\n",
        "%cd /content/Text-To-Video-Finetuning\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\"\"\"\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic=True\n",
        "random.seed(2)\n",
        "np.random.seed(2)\n",
        "torch.manual_seed(2)\n",
        "torch.cuda.manual_seed(2)\n",
        "torch.cuda.manual_seed_all(2)\n",
        "torch.manual_seed(0)\n",
        "\"\"\"\n",
        "#print(\"seed is \" + str(torch.seed()))\n",
        "\n",
        "#seeding = \"Random\"\n",
        "#thisSeed = 123;\n",
        "\n",
        "\n",
        "#preset = \"Manual\"\n",
        "# while True:\n",
        "\n",
        "prompt = \"extremely detailed, Futuristic Cityscape, blade runner, extremely cloudy, awardwinning, best quality, 8k\" #@param {type:\"string\"}\n",
        "negative = \"text, watermark, copyright, blurry, nsfw, noise, quick motion, bad quality, flicker, dirty, ugly, fast motion, quick cuts, fast editing, cuts, blurry\" #@param {type:\"string\"}\n",
        "prompt = f\"\\\"{prompt}\\\"\"\n",
        "negative = f\"\\\"{negative}\\\"\"\n",
        "num_steps = 25 #@param {type:\"raw\"}\n",
        "guidance_scale = 23 #@param {type:\"raw\"}\n",
        "fps = 10 #@param {type:\"raw\"}\n",
        "num_frames = 30 #@param {type:\"raw\"}\n",
        "seedManual = \"Random\"\n",
        "seeding = \"Random\" #@param [\"Random\", \"Manual\"]\n",
        "\n",
        "inputSeed = 5939699337684636079 #@param {type:\"raw\"}\n",
        "if seeding == \"Random\":\n",
        "  thisSeed = random.randint(0, ((1<<63)-1))\n",
        "  print(\"seed is \" + str(thisSeed))\n",
        "else:\n",
        "  thisSeed = inputSeed\n",
        "\n",
        "!python inference.py -m \"/content/zeroscope_v2_dark_30x448x256\" -p {prompt} -n {negative} -W 448 -H 256 -o /content/outputs -d cuda -x -s {num_steps} -g {guidance_scale} -f {fps} -T {num_frames} -seed {thisSeed}\n",
        "#-seed {thisSeed}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Iuu47h69khNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title V2V Step 2: Select Video (Upload from local drive or Gdrive)\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from IPython.display import HTML, clear_output\n",
        "from base64 import b64encode\n",
        "import moviepy.editor as mp\n",
        "\n",
        "\n",
        "def showVideo(file_path):\n",
        "    \"\"\"Function to display video in Colab\"\"\"\n",
        "    mp4 = open(file_path,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(\"\"\"\n",
        "    <video controls width=600>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url))\n",
        "\n",
        "def get_video_resolution(video_path):\n",
        "    \"\"\"Function to get the resolution of a video\"\"\"\n",
        "    import cv2\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    return (width, height)\n",
        "\n",
        "def resize_video(video_path, new_resolution):\n",
        "    \"\"\"Function to resize a video\"\"\"\n",
        "    import cv2\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    fourcc = int(video.get(cv2.CAP_PROP_FOURCC))\n",
        "    fps = video.get(cv2.CAP_PROP_FPS)\n",
        "    width, height = new_resolution\n",
        "    output_path = os.path.splitext(video_path)[0] + '_720p.mp4'\n",
        "    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "    while True:\n",
        "        success, frame = video.read()\n",
        "        if not success:\n",
        "            break\n",
        "        resized_frame = cv2.resize(frame, new_resolution)\n",
        "        writer.write(resized_frame)\n",
        "    video.release()\n",
        "    writer.release()\n",
        "\n",
        "#@markdown ### Select an uploading method\n",
        "upload_method = \"Custom Path\" #@param [\"Upload\", \"Custom Path\"]\n",
        "\n",
        "\n",
        "# remove previous input video\n",
        "if os.path.isfile('/content/sample_data/input_vid.mp4'):\n",
        "    os.remove('/content/sample_data/input_vid.mp4')\n",
        "\n",
        "if upload_method == \"Upload\":\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        os.rename(filename, '/content/sample_data/input_vid.mp4')\n",
        "    PATH_TO_YOUR_VIDEO = '/content/sample_data/input_vid.mp4'\n",
        "\n",
        "elif upload_method == 'Custom Path':\n",
        "    if not 'drive' in globals():\n",
        "        drive.mount('/content/drive')\n",
        "    #@markdown ``Add the full path to your video on your Gdrive `` üëá\n",
        "    PATH_TO_YOUR_VIDEO = '' #@param {type:\"string\"}\n",
        "    if not os.path.isfile(PATH_TO_YOUR_VIDEO):\n",
        "        print(\"ERROR: File not found!\")\n",
        "        raise SystemExit(0)\n",
        "\n",
        "#@markdown <font color=\"orange\">Notes:\n",
        "\n",
        "#@markdown <font color=\"orange\">. ``If your uploaded video is 1080p or higher resolution, this cell will resize it to 720p.``\n",
        "\n",
        "#@markdown <font color=\"orange\">. ``Do not upload videos longer than 60 seconds.``\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "video_duration = mp.VideoFileClip(PATH_TO_YOUR_VIDEO).duration\n",
        "if video_duration > 60:\n",
        "    print(\"WARNING: Video duration exceeds 60 seconds. Please upload a shorter video.\")\n",
        "    raise SystemExit(0)\n",
        "\n",
        "video_resolution = get_video_resolution(PATH_TO_YOUR_VIDEO)\n",
        "print(f\"Video resolution: {video_resolution}\")\n",
        "if video_resolution[0] >= 1920 or video_resolution[1] >= 1080:\n",
        "    print(\"Resizing video to 720p...\")\n",
        "    os.system(f\"ffmpeg -i {PATH_TO_YOUR_VIDEO} -vf scale=1280:720 /content/sample_data/input_vid.mp4\")\n",
        "    PATH_TO_YOUR_VIDEO = \"/content/sample_data/input_vid.mp4\"\n",
        "    print(\"Video resized to 720p\")\n",
        "else:\n",
        "    print(\"No resizing needed\")\n",
        "\n",
        "if upload_method == \"Upload\":\n",
        "  clear_output()\n",
        "  print(\"Input Video\")\n",
        "  showVideo(PATH_TO_YOUR_VIDEO)\n",
        "else:\n",
        "    if os.path.isfile(PATH_TO_YOUR_VIDEO):\n",
        "        shutil.copyfile(PATH_TO_YOUR_VIDEO, \"/content/sample_data/input_vid.mp4\")\n",
        "        print(\"Input Video\")\n",
        "        showVideo(PATH_TO_YOUR_VIDEO)\n"
      ],
      "metadata": {
        "id": "GsKGWi1knhYZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title V2V STEP 3: V2V with Potat1 (Will Need Interpolating)\n",
        "%cd /content/Text-To-Video-Finetuning\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\"\"\"\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic=True\n",
        "random.seed(2)\n",
        "np.random.seed(2)\n",
        "torch.manual_seed(2)\n",
        "torch.cuda.manual_seed(2)\n",
        "torch.cuda.manual_seed_all(2)\n",
        "torch.manual_seed(0)\n",
        "\"\"\"\n",
        "#print(\"seed is \" + str(torch.seed()))\n",
        "\n",
        "#seeding = \"Random\"\n",
        "#thisSeed = 123;\n",
        "\n",
        "\n",
        "#preset = \"Manual\"\n",
        "# while True:\n",
        "#@markdown ### Select the model (be sure to have installed in Step 2)\n",
        "model = \"potat1\" #@param [\"potat1\", \"zeroscope_v2_dark_30x448x256\", \"zeroscope_v2_567w\", \"zeroscope_v2_XL\"]\n",
        "video_weight = .3 #@param {type:\"raw\"}\n",
        "prompt = \"extremely detailed, Futuristic Cityscape, blade runner, extremely cloudy, awardwinning, best quality, 8k\" #@param {type:\"string\"}\n",
        "negative = \"blurry, text, watermark, copyright, blurry, nsfw, noise, quick motion, bad quality, flicker, dirty, ugly, fast motion, quick cuts, fast editing, cuts\" #@param {type:\"string\"}\n",
        "prompt = f\"\\\"{prompt}\\\"\"\n",
        "negative = f\"\\\"{negative}\\\"\"\n",
        "num_steps = 50 #@param {type:\"raw\"}\n",
        "guidance_scale = 23 #@param {type:\"raw\"}\n",
        "width = 600 #@param {type:\"raw\"}\n",
        "height = 336 #@param {type:\"raw\"}\n",
        "fps = 6 #@param {type:\"raw\"}\n",
        "num_frames = 18 #@param {type:\"raw\"}\n",
        "#@markdown ### Seeding not currently working for v2v (WIP)\n",
        "seedManual = \"Random\"\n",
        "seeding = \"Random\" #@param [\"Random\", \"Manual\"]\n",
        "\n",
        "inputSeed = 6708511088475640657 #@param {type:\"raw\"}\n",
        "if seeding == \"Random\":\n",
        "  thisSeed = random.randint(0, ((1<<63)-1))\n",
        "  print(\"seed is \" + str(thisSeed))\n",
        "else:\n",
        "  thisSeed = inputSeed\n",
        "\n",
        "thisHeight = int(round(height/8.0)*8.0)\n",
        "thisWidth = int(round(width/8.0)*8.0)\n",
        "thisModel=\"/content/\"+model\n",
        "!python inference.py -m {thisModel} -p {prompt} -n {negative} -W {thisWidth} -H {thisHeight} -o /content/outputs -d cuda -x -s {num_steps} -g {guidance_scale} -f {fps} -T {num_frames} -seed {thisSeed} -i \"/content/sample_data/input_vid.mp4\" -iw {video_weight}\n",
        "#-seed {thisSeed}"
      ],
      "metadata": {
        "id": "4mHF40hUptZT",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}